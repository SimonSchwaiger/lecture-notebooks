{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c2fc39-2d24-498b-8955-eaba7febe588",
   "metadata": {},
   "source": [
    "# SOAR Project Analysis\n",
    "\n",
    "## Graph Creation based on Occupancy Grid\n",
    "\n",
    "- **Occupancy Grid** as representation of a map in discrete form\n",
    "- Graph creation done based on sampling of occupancy grid\n",
    "- Graph represents information in occupancy grid\n",
    "\n",
    "**Required Steps**\n",
    "1. Determine possible neighbouring nodes\n",
    "2. Create the path to determined neighbours\n",
    "3. Check each cell between current node and neighbour to determine if neighour should be added to graph\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "    <center><img src=\"./material/notebookFigures-graphCreationFromOccupancyGrid.drawio.png\" alt=\"Graph Creation Concept\" style=\"width: 850px\"></center>\n",
    "    <center><i><b>Figure:</b> Dynamic graph creation is done in three steps per node </i></center></br></br>\n",
    "    </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"page-break-after: always;\"></div>\n",
    "<div>&nbsp;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848c6eb-bf77-4e2d-b174-9f1a1748f699",
   "metadata": {},
   "source": [
    "# Outlook on Sensor-Based Robot Control\n",
    "\n",
    "<div style=\"text-align: justify; max-width:1080px\">\n",
    "    Our project implementation incorporated a basic system of sensor-based robot control. In practice, however, applications are not as constrained as our simulation. This requires control applications to be much more sophisticated, incorporating statistic algorithms, machine learning, deep learning to accurately percieve the environment and determine robot behaviour. Here are some examples.\n",
    "    <h2>MoveIt Trajectory Planning using Multi-Stage Planning</h2>\n",
    "    The MoveIt framework implements trajectory planning using a pipeline of collision checks and search passes.\n",
    "    <center><img src=\"https://moveit.ros.org/documentation/applications/diabolo.gif\" alt=\"MoveIt Multi Arm demo\" style=\"width: 650px\"></center>\n",
    "    <center><img src=\"https://moveit.ros.org/documentation/applications/diabolosim.gif\" alt=\"MoveIt Multi Arm demo\" style=\"width: 650px\"></center>\n",
    "    <center><i><b>Figure:</b>Two 6DOF robots perform simultaneous trajectory planning using the MoveIt framework to play the game of Diabolo <a href=\"https://moveit.ros.org/documentation/applications/\">(Source)</a></i></center></br></br>\n",
    "    <h2>Object Detection using Deep Learning</h2>\n",
    "    Object detection refers to a model determining whether or not a certain object is in an image and if it is, the model also identifies the object and its position in the image.</br></br>\n",
    "    <center><img src=\"https://user-images.githubusercontent.com/90580636/162844601-5b23ccc4-eaec-4402-b841-1047a62ac1ee.gif\" alt=\"Object Detection for Autonomous Grasping\" style=\"width: 650px\"></center>\n",
    "    <center><i><b>Figure:</b>Objects are detected using the <a href=\"https://arxiv.org/abs/2004.10934\">YOLOv4 object detection model</a> in order to be grasped by a 6DOF robot arm <a href=\"https://github.com/Walid-khaled/YOLO-Object-Detection-for-Pick-and-Place-task-using-ROS-on-KUKA-iiwa\">(Source)</a></i></center></br></br>\n",
    "    <center><iframe width=\"700\" height=\"700\" src=\"https://www.youtube.com/embed/7nD9INKZuLk\" title=\"YOLO Object Detection for Pick and Place task using ROS on KUKA iiwa\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></center>\n",
    "    <h2>Segmentation/Detection and Pose Estimation using Deep Learning</h2>\n",
    "    Modern detectors are able to perform multiple tasks simultaneously. An example for such a model is Facebook's <a href=\"https://github.com/facebookresearch/detectron2\">Detectron2</a>, which performs detection, semantic segmentation and human pose estimation simultaneously. Semantic segmentation refers to the act of partitioning an image in multiple regions (e.g. sky vs. road).</br></br>\n",
    "    <center><img src=\"https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png\" alt=\"Detectron2 Demo\" style=\"width: 650px\"></center>\n",
    "    <center><i><b>Figure:</b> Detection of Objects, Humans, Image Regions as well as Poses <a href=\"https://github.com/facebookresearch/detectron2\">(Source)</a></i></center></br></br>\n",
    "    <h2>Task Planning using Reinforcement Learning</h2>\n",
    "    Reinforcement learning is a subdomain of machine learning, where an agent learns based on sampled training data and a feedback signal. Based on the chosen algorithm, simple search problems up to complex tasks such as autonomous driving can be solved using reinforcement learning. This demo shows how reinforcement learning models can start learning from a reference; in this case a robot dog learns from a real dog in simulation.</br></br>\n",
    "    <center><img src=\"https://bair.berkeley.edu/static/blog/laikago/00_teaser.gif\" alt=\"Robot Dog RL Gait Demo\" style=\"width: 650px\"></center>\n",
    "    <center><img src=\"https://bair.berkeley.edu/static/blog/laikago/05_sim_pace.gif\" alt=\"Robot Dog RL Gait Demo\" style=\"width: 325px\"><img src=\"https://bair.berkeley.edu/static/blog/laikago/06_sim_spin.gif\" alt=\"Robot Dog RL Gait Demo\" style=\"width: 325px\"></center>\n",
    "     <center><i><b>Figure:</b> Gait viewed as a control problem can be solved using reinforcement learning <a href=\"https://bair.berkeley.edu/blog/2020/04/03/laikago/\">(Source)</a></i></center></br>\n",
    "    <h2>Task Planning using Natural Language Processing</h2>\n",
    "    Researchers at Google have combined natural language processing with robot control to impelment a pipeline where a robot learns to perform tasks solely from a textual task description as well as sensor data.</br></br>\n",
    "    <center><video width=\"650\" height=\"366\" autoplay controls loop muted>\n",
    "      <source src=\"https://robotics-transformer.github.io/img/mosaic_saycan_comp.mp4\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video></center>\n",
    "    <center><i><b>Figure:</b>The robot has been trained in various scenarios in simulation as well as the real world <a href=\"https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html\">(Source)</a></i></center></br></br>\n",
    "    <h2>Model Predictive Robot Control</h2>\n",
    "    Model predictive control uses forward simulation on a mathematical model of the robot dynamics to simulate multiple control configurations for each time step, before choosing the best step to perform. Boston Dynamics uses this technique to predict how movement of their Atlas robot impacts the environment.</br></br>\n",
    "    <center><iframe width=\"650\" height=\"366\" src=\"https://www.youtube.com/embed/-e1_QhJ1EhQ\" title=\"Atlas Gets a Grip | Boston Dynamics\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></center>\n",
    "    <center><i><b>Figure:</b> Showcase of Atlas' new control system incorporating movement and interaction with objects <a href=\"https://www.youtube.com/watch?v=-e1_QhJ1EhQ\">(Source)</a></i></center></br></br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186e348-343f-45df-a4ec-4d3e9f388b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
